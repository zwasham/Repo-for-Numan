{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steward View\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this program is to support the good stewardship of the resources that God has entrusted to me by providing me with a clear view of the historical financial transactions of my household."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local packages.\n",
    "from constants import PATH_DICT, SPECIAL_SUBSTRINGS    # Because the names and locations of the demo data will never change, \n",
    "from functions import update_columns, label_payment_data, label_product_data, display_pie_chart, display_bar_chart\n",
    "\n",
    "# Import Python Standard Library packages.\n",
    "from logging import basicConfig, debug, DEBUG, WARNING     # Remove this from the public version.\n",
    "basicConfig(level=WARNING, format='%(message)s')           # Remove this from the public version.\n",
    "from datetime import timedelta #, date\n",
    "from itertools import combinations\n",
    "\n",
    "# Import 3rd party packages.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zach's Wells Fargo Checking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read my Wells Fargo checking data into the program.\n",
    "column_names = ['Date', 'Amount', '*', 'Check Number', 'Description']\n",
    "z_wf_ch = pd.read_csv(\n",
    "    PATH_DICT[\"Zach's WF Checking Transactions\"], \n",
    "    header = None, \n",
    "    names = column_names, \n",
    "    usecols = ['Date', 'Amount', 'Description']\n",
    "    )\n",
    "\n",
    "# Set the data type of the existing Date, Amount, and Description columns and initialize new columns.\n",
    "z_wf_ch = update_columns(z_wf_ch, 'Zach', 'WF Checking')\n",
    "\n",
    "z_wf_ch.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read my Wells Fargo checking data into the program.\n",
    "column_names = ['Date', 'Amount', '*', 'Check Number', 'Description']\n",
    "z_wf_ch = pd.read_csv(\n",
    "    PATH_DICT[\"Zach's WF Checking Transactions\"], \n",
    "    header = None, \n",
    "    names = column_names, \n",
    "    usecols = ['Date', 'Amount', 'Description']\n",
    "    )\n",
    "\n",
    "# Set the data type of the existing Date, Amount, and Description columns and initialize new columns.\n",
    "z_wf_ch = update_columns(z_wf_ch, 'Zach', 'WF Checking')\n",
    "\n",
    "# Extract income transactions.\n",
    "z_income_transactions = z_wf_ch[z_wf_ch['Description'].str.contains(SPECIAL_SUBSTRINGS['z']['income'], na=False)]\n",
    "z_income_transactions = z_income_transactions.reset_index(drop = True)\n",
    "z_income_transactions.to_csv(f'Data/3. Final/{TODAY} Zach Income.csv', index = False)\n",
    "z_wf_ch = z_wf_ch[~z_wf_ch['Description'].str.contains(SPECIAL_SUBSTRINGS['z']['income'], na=False)]\n",
    "\n",
    "\n",
    "# Exclude unwanted transactions.\n",
    "z_wf_ch = z_wf_ch[~z_wf_ch['Description'].str.contains(SPECIAL_SUBSTRINGS['z']['exclusion'], na=False)]\n",
    "\n",
    "# Reset the DataFrame index.\n",
    "z_wf_ch = z_wf_ch.reset_index(drop = True)\n",
    "\n",
    "# Label transactions.\n",
    "simple_labeler(z_wf_ch, vendor_dict)\n",
    "llm_labeler(z_wf_ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Emily's Bank of America Checking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Emily's Bank of America checking data into the program.\n",
    "e_ba_ch = pd.read_csv(PATH_DICT[\"Emily's B of A Checking Transactions\"], header = 6)\n",
    "\n",
    "# Drop the unnecessary column, set the data type of the existing Date, Amount, and Description columns, and \n",
    "# initialize new columns.\n",
    "e_ba_ch = update_columns(e_ba_ch, 'Emily', 'BA Checking', drop = ['Running Bal.'])\n",
    "\n",
    "# Remove unwanted transactions.\n",
    "e_ba_ch = e_ba_ch[~e_ba_ch['Description'].str.contains(SPECIAL_SUBSTRINGS['e']['exclusion'], na=False)]\n",
    "e_ba_ch = e_ba_ch.reset_index(drop=True)\n",
    "\n",
    "# Label transactions.\n",
    "simple_labeler(e_ba_ch, vendor_dict)\n",
    "llm_labeler(e_ba_ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Emily's Bank of America Credit Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Emily's Bank of America credit card data into the program.\n",
    "e_ba_cc = pd.read_csv(PATH_DICT[\"Emily's B of A Credit Card Transactions\"])\n",
    "\n",
    "# Drop unnecessary columns, rename columns as needed, set the data type of the existing Date, Amount, and \n",
    "# Description columns, and initialize new columns.\n",
    "e_ba_cc_drop = [\n",
    "    'Reference',\n",
    "    'Address'\n",
    "]\n",
    "e_ba_cc_rename = {\n",
    "    'Posted Date' : 'Date',\n",
    "    'Payee' : 'Description'\n",
    "}\n",
    "e_ba_cc = update_columns(e_ba_cc, 'Emily', 'BA Credit Card', drop = e_ba_cc_drop, rename = e_ba_cc_rename)\n",
    "\n",
    "# Remove unwanted transactions.\n",
    "e_ba_ch = e_ba_ch[~e_ba_ch['Description'].str.contains(SPECIAL_SUBSTRINGS['e']['exclusion'], na=False)]\n",
    "e_ba_ch = e_ba_ch.reset_index(drop=True)\n",
    "\n",
    "# Label transactions.\n",
    "simple_labeler(e_ba_cc, vendor_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Emily's Chase Credit Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Emily's Chase credit card data into the program.\n",
    "e_chase_cc = pd.read_csv('Data/x. Test/Emily Chase Credit Card.csv')\n",
    "\n",
    "# Drop the Post Date, Type, and Memo columns, rename the Transaction Date column to Date, set the data type\n",
    "# of the Date, Amount, and Description columns, and initialize new columns (except for Category, which is\n",
    "# already present).\n",
    "e_chase_cc = update_columns(\n",
    "    e_chase_cc, \n",
    "    'Emily', \n",
    "    'Chase Credit Card', \n",
    "    columns_to_drop = ['Post Date', 'Type', 'Memo'], \n",
    "    columns_to_rename = {'Transaction Date' : 'Date'}, \n",
    "    do_not_initialize = ['Category']\n",
    "    )\n",
    "\n",
    "# Remove unwanted transactions.\n",
    "e_chase_cc = e_chase_cc[~e_chase_cc['Description'].str.contains(SPECIAL_SUBSTRINGS['e']['exclusion'], na=False)].copy()\n",
    "e_chase_cc = e_chase_cc.reset_index(drop=True)\n",
    "\n",
    "# Update the default Chase category names with the simple_labeler and the chase_category_map.\n",
    "e_chase_cc = label_payment_data(e_chase_cc)\n",
    "e_chase_cc = update_chase_categories(e_chase_cc)\n",
    "e_chase_cc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Emily's Chase credit card data into the program.\n",
    "e_chase_cc = pd.read_csv(PATH_DICT[\"Emily's Chase Credit Card Transactions\"])\n",
    "\n",
    "# Drop the Post Date, Type, and Memo columns, rename the Transaction Date column to Date, set the data type\n",
    "# of the Date, Amount, and Description columns, and initialize new columns (except for Category, which is\n",
    "# already present).\n",
    "e_chase_cc = update_columns(\n",
    "    e_chase_cc, \n",
    "    'Emily', \n",
    "    'Chase Credit Card', \n",
    "    drop = ['Post Date', 'Type', 'Memo'], \n",
    "    rename = {'Transaction Date' : 'Date'}, \n",
    "    do_not_initialize = ['Category']\n",
    "    )\n",
    "\n",
    "# Remove unwanted transactions.\n",
    "e_chase_cc = e_chase_cc[~e_chase_cc['Description'].str.contains(SPECIAL_SUBSTRINGS['e']['exclusion'], na=False)].copy()\n",
    "e_chase_cc = e_chase_cc.reset_index(drop=True)\n",
    "\n",
    "# Update the default Chase category names with the simple_labeler and the chase_category_map.\n",
    "simple_labeler(e_chase_cc, vendor_dict)\n",
    "e_chase_cc = convert_chase_labels(e_chase_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Amazon Purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into the program.\n",
    "amazon_data = pd.read_csv(\n",
    "    'Data/x. Test/Amazon Order History.csv', \n",
    "    usecols = ['Order Date', 'Total Owed', 'Payment Instrument Type', 'Order Status', 'Product Name']\n",
    "    )\n",
    "\n",
    "amazon_column_dict = {\n",
    "    'Order Date' : 'Date',\n",
    "    'Total Owed' : 'Amount',\n",
    "    'Product Name' : 'Description'\n",
    "}\n",
    "\n",
    "amazon_data = update_columns(amazon_data, spender = 'Emily', account = 'Amazon', columns_to_rename = amazon_column_dict)\n",
    "\n",
    "amazon_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_product_data(amazon_data)\n",
    "amazon_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into the program.\n",
    "amazon_data = pd.read_csv(\n",
    "    PATH_DICT['Amazon Order History'], \n",
    "    usecols = ['Order Date', 'Total Owed', 'Payment Instrument Type', 'Order Status', 'Product Name']\n",
    "    )\n",
    "\n",
    "# Remove transactions that occurred prior to the cutoff date. This will also remove rows with an Order Date value of\n",
    "# NaT (i.e. cancelled orders), resulting in a non-contiguous index; therefore, reset the index.\n",
    "amazon_data['Order Date'] = pd.to_datetime(amazon_data['Order Date'], errors = 'coerce').dt.tz_localize(None)    # Passing this column to the parse_dates parameter of the read_csv function fails. The tz_localize method removes the timezone data so that the values in this column are comparable with cutoff_date.\n",
    "amazon_data = amazon_data[amazon_data['Order Date'] >= CUTOFF_DATE]\n",
    "amazon_data = amazon_data.reset_index(drop = True)\n",
    "\n",
    "# Add Spender, Account, and vendor columns.\n",
    "amazon_data['Spender'] = 'Emily'\n",
    "amazon_data['Account'] = 'Amazon'\n",
    "amazon_data['Vendor'] = 'Amazon'\n",
    "\n",
    "# Rename relevant columns to match the Chase column names (for easier concatenation).\n",
    "amazon_column_dict = {\n",
    "    'Order Date' : 'Date',\n",
    "    'Total Owed' : 'Amount',\n",
    "    'Product Name' : 'Description'\n",
    "}\n",
    "for old_name, new_name in amazon_column_dict.items():\n",
    "    amazon_data = amazon_data.rename(columns = {old_name : new_name})\n",
    "\n",
    "# Classify each product purchase with the llm_labeler.\n",
    "llm_labeler(amazon_data, amazon = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all Amount column values to the Decimal type with two decimal places.\n",
    "amazon_data['Amount'] = amazon_data['Amount'].astype(str).map(lambda value: Decimal(value).quantize(Decimal('0.01')))\n",
    "\n",
    "# Switch all Amount values from positive to negative (to match the Chase data).\n",
    "amazon_data['Amount'] *= -1\n",
    "\n",
    "# Separate and export transactions that involved payment methods other than Emily's Chase credit card.\n",
    "amazon_data['Payment Instrument Type'] = amazon_data['Payment Instrument Type'].astype('string')\n",
    "chase_mask = amazon_data['Payment Instrument Type'].str.contains(LAST_FOUR) & ~amazon_data['Payment Instrument Type'].str.contains('and')    # From this point until the Amazon product purchase data is added to the Chase DataFrame and the redundant Chase data is removed, row indices must remain the same to avoid errors.\n",
    "amazon_data_chase = amazon_data[chase_mask]\n",
    "amazon_data_chase = amazon_data_chase.reset_index(drop = True)\n",
    "amazon_data_other = amazon_data[~chase_mask]\n",
    "amazon_data_other.to_csv(f'Data/3. Final/{TODAY} Amazon Order History (Other).csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the Amazon transactions within the Chase data. Using loc prevents index reset.\n",
    "e_chase_cc_amazon = e_chase_cc.loc[e_chase_cc['Vendor'] == 'Amazon']\n",
    "\n",
    "# Initialize a list for storing Amazon orders that have no match in the Chase data.\n",
    "unmatched_orders = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### [Temporary: Export and re-read all data files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all data files\n",
    "z_wf_ch.to_csv(f'Data/2. Intermediate/{TODAY} z_wf_ch.csv', index = False)\n",
    "e_ba_ch.to_csv(f'Data/2. Intermediate/{TODAY} e_ba_ch.csv', index = False)\n",
    "e_ba_cc.to_csv(f'Data/2. Intermediate/{TODAY} e_ba_cc.csv', index = False)\n",
    "e_chase_cc.to_csv(f'Data/2. Intermediate/{TODAY} e_chase_cc.csv', index = False)\n",
    "e_chase_cc_amazon.to_csv(f'Data/2. Intermediate/{TODAY} e_chase_cc_amazon.csv', index = False)\n",
    "amazon_data.to_csv(f'Data/2. Intermediate/{TODAY} amazon_data.csv', index = False)\n",
    "amazon_data_chase.to_csv(f'Data/2. Intermediate/{TODAY} amazon_data_chase.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-read all data files into the program.\n",
    "# If the date in the names of the files that you want to read into the program is today's date, then replace the \n",
    "# date in the f-strings below with {TODAY}.\n",
    "z_wf_ch = pd.read_csv(f'Data/2. Intermediate/2025.05.15 z_wf_ch.csv')\n",
    "e_ba_ch = pd.read_csv(f'Data/2. Intermediate/2025.05.15 e_ba_ch.csv')\n",
    "e_ba_cc = pd.read_csv(f'Data/2. Intermediate/2025.05.15 e_ba_cc.csv')\n",
    "e_chase_cc = pd.read_csv(f'Data/2. Intermediate/2025.05.15 e_chase_cc.csv')\n",
    "amazon_data = pd.read_csv(f'Data/2. Intermediate/2025.05.15 amazon_data.csv')\n",
    "amazon_data_chase = pd.read_csv(f'Data/2. Intermediate/2025.05.15 amazon_data_chase.csv')\n",
    "\n",
    "# Isolate the Amazon transactions within the Chase data. Using loc prevents index reset.\n",
    "e_chase_cc_amazon = e_chase_cc.loc[e_chase_cc['Vendor'] == 'Amazon'].copy()\n",
    "\n",
    "unmatched_orders = []\n",
    "\n",
    "dfs = [\n",
    "    z_wf_ch,\n",
    "    e_ba_ch,\n",
    "    e_ba_cc,\n",
    "    e_chase_cc,\n",
    "    e_chase_cc_amazon,\n",
    "    amazon_data,\n",
    "    amazon_data_chase,\n",
    "]\n",
    "\n",
    "for prepared_data in dfs:\n",
    "    prepared_data['Amount'] = prepared_data['Amount'].astype(str).map(lambda value: Decimal(value).quantize(Decimal('0.01')))\n",
    "    prepared_data['Date'] = pd.to_datetime(prepared_data['Date'], errors='coerce')\n",
    "    print(type(prepared_data.loc[0, 'Amount']))\n",
    "    print(type(prepared_data.loc[0, 'Date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Amazon transactions in the Chase data with the associated individual product purchases from the \n",
    "# Amazon data.\n",
    "for order in amazon_data_chase['Date'].unique():\n",
    "    debug(f'Order = {order}')\n",
    "    \n",
    "    # Define a DataFrame containing all of the product purchases in the order.\n",
    "    all_order_rows = amazon_data_chase[amazon_data_chase['Date'] == order].copy()    # The copy method ensures that all_order_rows is a distinct DataFrame, not merely a view.\n",
    "    debug(f'all_order_rows:\\n{all_order_rows[['Date', 'Amount']]}')\n",
    "    \n",
    "    # Gather the indices of all Amazon transactions in the Chase data with a date equal to or one day after\n",
    "    # order.date().\n",
    "    date_match_indices = []\n",
    "    date_match_indices_to_remove = []    # When the program identifies a Chase transaction with a matching Amount while looping through date_match_indices, it will store the index of the matching transaction in date_match_indices_to_remove for removal from date_match_indices after the completion of the loop that is iterating through date_match_indices. Removing matched indices during the loop rather than after the completion of the loop would cause the loop to skip elements of the iterable (because such removal would change the positions of the iterable elements without any change to the internal for loop position counter).\n",
    "    for index in e_chase_cc_amazon.index:\n",
    "        chase_date = e_chase_cc_amazon.loc[index, 'Date'].date()    # The date function converts these objects from pandas Timestamps (which include time of day) to PSL dates (which do not).\n",
    "        order_date = order.date()\n",
    "\n",
    "        # If the date of any transaction in the Chase data is equal to or up to two days greater than order_date,\n",
    "        # then append the index of that transaction to date_match_indices.\n",
    "        if chase_date in [order_date, order_date + timedelta(days = 1), order_date + timedelta(days = 2)]:\n",
    "            date_match_indices.append(index)\n",
    "    \n",
    "    # If no transaction in the Chase data has a matching date, then append the individual product purchase data\n",
    "    # to unmatched_orders and proceed to the next order.\n",
    "    if len(date_match_indices) == 0:\n",
    "        debug(f'No matching Chase transactions for order {order}.')\n",
    "        unmatched_orders.append(all_order_rows)\n",
    "        debug(f'Appending unmatched order:\\n{all_order_rows[['Date', 'Amount']]}\\nto unmatched_orders.')\n",
    "        continue\n",
    "    \n",
    "    debug(f'Matching Chase transactions for order {order}:\\n{date_match_indices}')\n",
    "    debug(f'Number of matching Chase transactions: {len(date_match_indices)}')\n",
    "\n",
    "    # Attempt to match product purchase rows in the Amazon data to transactions in the Chase data.\n",
    "    all_order_rows_matched = False\n",
    "    \n",
    "    # For each Chase transaction with a date match, try matching the 'Amount' value of the whole Amazon\n",
    "    # order.\n",
    "    total_order_cost = all_order_rows['Amount'].sum()\n",
    "    debug(f'Total order cost: {total_order_cost}')\n",
    "    for chase_index in date_match_indices:\n",
    "        chase_amount = e_chase_cc.loc[chase_index, 'Amount']\n",
    "        debug(f'Chase Amount: {chase_amount}')\n",
    "        if chase_amount == total_order_cost:\n",
    "            debug(f'Chase Amount matches total order cost for order {order}.')\n",
    "            all_order_rows_matched = True\n",
    "\n",
    "            # Reset the index of all_order_rows to prepare for concatenation.\n",
    "            new_starting_index = e_chase_cc.index.max() + 1\n",
    "            debug(f'Final index of e_chase_cc: {e_chase_cc.index.max()}')\n",
    "            debug(f'New starting index for all_order_rows: {new_starting_index}')\n",
    "            all_order_rows.index = range(new_starting_index, new_starting_index + len(all_order_rows))\n",
    "            debug(f'Updated all_order_rows index:\\n{all_order_rows.index}')\n",
    "\n",
    "            # Add all_order_rows to e_chase_cc.\n",
    "            e_chase_cc = pd.concat([e_chase_cc, all_order_rows], join = 'outer')    # In a future iteration, I could gather matched rows in a list and add them to e_chase_cc at the end of the order loop with a single concat function call.\n",
    "            debug(f'Added all_order_rows to e_chase_cc:\\n{e_chase_cc.tail(len(all_order_rows))[['Date', 'Amount']]}')\n",
    "\n",
    "            # Drop the matched Chase transaction from both Chase DataFrames.\n",
    "            e_chase_cc = e_chase_cc.drop(index = chase_index)    # The drop method never returns a view, so calling the copy method is unnecessary.\n",
    "            debug(f'Dropped matched Chase transaction (index {chase_index}) from e_chase_cc.')\n",
    "            e_chase_cc_amazon = e_chase_cc_amazon.drop(index = chase_index)\n",
    "            debug(f'Dropped matched Chase transaction (index {chase_index}) from e_chase_cc_amazon.')\n",
    "\n",
    "            # Break out of the chase_index loop.\n",
    "            debug(f'Breaking out of the chase_index loop because the whole order has been matched.')\n",
    "            break\n",
    "\n",
    "    # If the prior loop matched the whole Amazon order to a Chase transaction, then proceed to the \n",
    "    # next order.\n",
    "    if all_order_rows_matched:\n",
    "        debug(f'All product purchases in order {order} have been matched to a Chase transaction. Proceeding to the next order.')\n",
    "        continue\n",
    "\n",
    "    # If the Amazon order contains only one product and the prior for loop did not find a match, then \n",
    "    # there is no match. Add the unmatched row to unmatched_orders and continue to the next index.\n",
    "    elif len(all_order_rows) == 1:\n",
    "        debug(f'Order {order} contains only one product purchase which has not been matched to a Chase transaction. Adding unmatched row to unmatched_orders.')\n",
    "        unmatched_orders.append(all_order_rows)\n",
    "        debug(f'unmatched_orders:\\n{unmatched_orders[-1][['Date', 'Amount']]}')\n",
    "        debug(f'Proceeding to the next order.')\n",
    "        continue\n",
    "\n",
    "    # Try matching the 'Amount' value of each individual product in the Amazon order.\n",
    "    debug(f'Attempting to match individual product purchases in order {order} to Chase transactions.')\n",
    "    product_purchase_indices_to_remove = []\n",
    "    changes_made = False\n",
    "    for chase_index in date_match_indices:\n",
    "        debug(f'Chase Index: {chase_index}')\n",
    "        for product_purchase_index in all_order_rows.index:\n",
    "            \n",
    "            # Extract the product purchase row as a DataFrame (for easier concatenation).\n",
    "            product_purchase = all_order_rows.loc[[product_purchase_index]].copy()\n",
    "            debug(f'Product Purchase: {product_purchase[['Date', 'Amount']]}')\n",
    "\n",
    "            # Define Amount variables for comparison.\n",
    "            chase_amount = e_chase_cc.loc[chase_index, 'Amount']\n",
    "            debug(f'Chase Amount: {chase_amount}')\n",
    "            product_purchase_amount = all_order_rows.loc[product_purchase_index, 'Amount']\n",
    "            debug(f'Product Purchase Amount: {product_purchase_amount}')\n",
    "\n",
    "            if chase_amount == product_purchase_amount:\n",
    "                debug(f'Chase Amount matches Product Purchase Amount.')\n",
    "                changes_made = True\n",
    "                debug(f'The value of changes_made has been set to True.')\n",
    "\n",
    "                # Update the index of product_purchase to prepare for concatenation.\n",
    "                new_index = e_chase_cc.index.max() + 1\n",
    "                debug(f'Final index of e_chase_cc: {e_chase_cc.index.max()}')\n",
    "                debug(f'New index for product_purchase: {new_index}')\n",
    "                product_purchase.index = [new_index]\n",
    "                debug(f'Updated product_purchase index:\\n{product_purchase.index}')\n",
    "\n",
    "                # Add product_purchase to e_chase_cc.\n",
    "                e_chase_cc = pd.concat([e_chase_cc, product_purchase], join = 'outer')\n",
    "                debug(f'Added product_purchase to e_chase_cc:\\n{e_chase_cc.tail(1)[['Date', 'Amount']]}')\n",
    "\n",
    "                # Drop the matched Chase transaction from both Chase DataFrames.\n",
    "                e_chase_cc = e_chase_cc.drop(index = chase_index)\n",
    "                debug(f'Dropped matched Chase transaction (index {chase_index}) from e_chase_cc.')\n",
    "                e_chase_cc_amazon = e_chase_cc_amazon.drop(index = chase_index)\n",
    "                debug(f'Dropped matched Chase transaction (index {chase_index}) from e_chase_cc_amazon.')\n",
    "\n",
    "                # Append the index of the matched product_purchase row to product_purchase_indices_to_remove\n",
    "                # (in case dropping the row from all_order_rows within this loop would break the loop, which\n",
    "                # I have not tested).\n",
    "                # Update: I just tested the above case, and it does not break the loop. For simplicity, I \n",
    "                # should revert to dropping the product purchase row from all_order_rows inside the loop. \n",
    "                product_purchase_indices_to_remove.append(product_purchase_index)\n",
    "                debug(f'Product purchase index {product_purchase_index} has been appended to product_purchase_indices_to_remove.')\n",
    "\n",
    "                # Append the matched index to date_match_indices_to_remove.\n",
    "                date_match_indices_to_remove.append(chase_index)\n",
    "                debug(f'Chase index {chase_index} has been appended to date_match_indices_to_remove.')\n",
    "                debug(f'Breaking out of the product_purchase_index loop.')\n",
    "\n",
    "                # Break out of the product_purchase_index loop.\n",
    "                break\n",
    "        \n",
    "        # Remove all matched product purchase rows from all_order_rows.\n",
    "        debug(f'Removing matched product purchase rows from all_order_rows.')\n",
    "        for i in product_purchase_indices_to_remove:    # Revert to the prior solution.\n",
    "            all_order_rows = all_order_rows.drop(index = i)    \n",
    "        debug(f'len(all_order_rows):\\n{len(all_order_rows)}')\n",
    "\n",
    "        # Clear the list for the next iteration of the chase_index loop.\n",
    "        product_purchase_indices_to_remove.clear()    \n",
    "        debug(f'Cleared product_purchase_indices_to_remove.')\n",
    "\n",
    "        # If the most recent iteration matched the final product purchase in all_order_rows, then flag \n",
    "        # all_order_rows_matched and break out of the chase_index loop.\n",
    "        if len(all_order_rows) == 0:\n",
    "            debug(f'All product purchases in order {order} have been matched to Chase transactions.')\n",
    "            all_order_rows_matched = True\n",
    "            debug(f'The value of all_order_rows_matched has been set to True.')\n",
    "            debug(f'Breaking out of the chase_index loop.')\n",
    "            break\n",
    "    \n",
    "        # If the prior loop matched at least one product purchase in the order, then test the combined \n",
    "        # Amount of all remaining product purchases in all_order_rows against the Amount of each Chase \n",
    "        # transaction.\n",
    "        elif changes_made:\n",
    "            debug(f'Changes have been made. Testing the combined Amount of all remaining product purchases in all_order_rows against the Amount of each Chase transaction.')\n",
    "            debug(f'len(all_order_rows): {len(all_order_rows)}')\n",
    "            total_remaining_order_amount = all_order_rows['Amount'].sum()\n",
    "            debug(f'total_remaining_order_amount: {total_remaining_order_amount}')\n",
    "            for chase_index_2 in date_match_indices:\n",
    "                debug(f'chase_index_2: {chase_index_2}')\n",
    "\n",
    "                # Look for chase_index_2 in e_chase_cc.index. If it is there, then assign the amount\n",
    "                # of the corresponding row to chase_amount_2. If it is not, then the corresponding row\n",
    "                # has been removed from the DataFrame because it was matched. In this case, proceed to\n",
    "                # the next index.\n",
    "                if chase_index_2 in e_chase_cc.index:\n",
    "                    debug(f'chase_index_2 is in e_chase_cc.index.')\n",
    "                    chase_amount_2 = e_chase_cc.loc[chase_index_2, 'Amount']    \n",
    "                    debug(f'chase_amount_2: {chase_amount_2}')\n",
    "                else:\n",
    "                    debug(f'chase_index_2 is not in e_chase_cc.index. Proceed to the next index in date_match_indices.')\n",
    "                    continue\n",
    "\n",
    "                # If there is a match, update the indicator variable and modify the data accordingly.\n",
    "                if chase_amount_2 == total_remaining_order_amount:\n",
    "                    debug(f'chase_amount_2 == total_remaining_order_amount')\n",
    "                    all_order_rows_matched = True\n",
    "                    debug(f'The value of all_order_rows_matched has been set to True.')\n",
    "\n",
    "                    # Add all product purchase rows remaining in all_order_rows to e_chase_cc.\n",
    "                    debug(f'Adding all remaining product purchase rows in all_order_rows to e_chase_cc.')\n",
    "                    for product_purchase_index in all_order_rows.index:\n",
    "                        product_df = all_order_rows.loc[[product_purchase_index]].copy()\n",
    "                        debug(f'product_df:\\n{product_df[['Date', 'Amount']]}')\n",
    "                        debug(f'Final index of e_chase_cc: {e_chase_cc.index.max()}')\n",
    "                        new_index = e_chase_cc.index.max() + 1\n",
    "                        debug(f'New index for product_df: {new_index}')\n",
    "                        product_df.index = [new_index]\n",
    "                        e_chase_cc = pd.concat([e_chase_cc, product_df], join = 'outer')\n",
    "                        debug(f'Added product_df to e_chase_cc:\\n{e_chase_cc.tail(1)[['Date', 'Amount']]}')\n",
    "                    debug(f'All remaining product purchase rows have been added to e_chase_cc.')\n",
    "\n",
    "                    # Clear all rows from all_order_rows.\n",
    "                    debug(f'Clearing all rows from all_order_rows.')\n",
    "                    all_order_rows = all_order_rows.drop(index = all_order_rows.index)\n",
    "                    debug(f'len(all_order_rows): {len(all_order_rows)}')\n",
    "\n",
    "                    # Drop the matched Chase transaction from both Chase DataFrames.\n",
    "                    e_chase_cc = e_chase_cc.drop(index = chase_index_2)\n",
    "                    debug(f'Dropped matched Chase transaction (index {chase_index_2}) from e_chase_cc.')\n",
    "                    e_chase_cc_amazon = e_chase_cc_amazon.drop(index = chase_index_2)\n",
    "                    debug(f'Dropped matched Chase transaction (index {chase_index_2}) from e_chase_cc_amazon.')\n",
    "\n",
    "                    # Break out of the chase_index_2 loop.\n",
    "                    debug(f'Breaking out of the chase_index_2 loop because the whole order has been matched.')\n",
    "                    break\n",
    "        \n",
    "        # If the prior loop matched the the remaining product purchases in all_order_rows, then break \n",
    "        # out of the chase_index loop.\n",
    "        if all_order_rows_matched:\n",
    "            debug(f'Breaking out of the chase_index loop because all product purchases in order {order} have been matched to Chase transactions.')\n",
    "            break\n",
    "\n",
    "    # If the prior loop matched all product purchases in the order, then proceed to the next Amazon order.\n",
    "    if all_order_rows_matched:\n",
    "        debug(f'All product purchases in order {order} have been matched to Chase transactions. Proceeding to the next order.')\n",
    "        continue\n",
    "\n",
    "    # Otherwise...\n",
    "    else:\n",
    "        debug(f'Not all product purchases in order {order} have been matched to Chase transactions. Proceeding to removing the indices of matched Chase transactions from date_match_indices.')\n",
    "        # remove any matched indices from date_match_indices and clear date_match_indices_to_remove, and\n",
    "        for i in date_match_indices_to_remove:\n",
    "            debug(f'Removing matched index {i} from date_match_indices.')\n",
    "            date_match_indices.remove(i)\n",
    "        debug(f'Removed all matched indices from date_match_indices:\\n{date_match_indices}')\n",
    "        date_match_indices_to_remove.clear()\n",
    "        debug(f'Cleared date_match_indices_to_remove.')\n",
    "    \n",
    "        # ...if the prior loop matched at least one product purchase in the order, then test the combined\n",
    "        # Amount of all remaining product purchases in all_order_rows against the Amount of each Chase\n",
    "        # transaction.\n",
    "        if changes_made:\n",
    "            debug(f'Changes have been made. Testing the combined Amount of all remaining product purchases in all_order_rows against the Amount of each Chase transaction.')\n",
    "            total_remaining_order_amount = all_order_rows['Amount'].sum()\n",
    "            debug(f'total_remaining_order_amount: {total_remaining_order_amount}')\n",
    "            for chase_index in date_match_indices:\n",
    "                debug(f'chase_index: {chase_index}')\n",
    "                chase_amount = e_chase_cc.loc[chase_index, 'Amount']\n",
    "                debug(f'chase_amount: {chase_amount}')\n",
    "\n",
    "                # If there is a match, update the indicator variable and modify the data accordingly.\n",
    "                if chase_amount == total_remaining_order_amount:\n",
    "                    debug(f'chase_amount == total_remaining_order_amount')\n",
    "                    all_order_rows_matched = True\n",
    "                    debug(f'The value of all_order_rows_matched has been set to True.')\n",
    "\n",
    "                    # Add all product purchase rows remaining in all_order_rows to e_chase_cc.\n",
    "                    debug(f'Adding all remaining product purchase rows in all_order_rows to e_chase_cc.')\n",
    "                    for product_purchase_index in all_order_rows.index:\n",
    "                        product_df = all_order_rows.loc[[product_purchase_index]].copy()\n",
    "                        debug(f'product_df:\\n{product_df[['Date', 'Amount']]}')\n",
    "                        debug(f'Final index of e_chase_cc: {e_chase_cc.index.max()}')\n",
    "                        new_index = e_chase_cc.index.max() + 1\n",
    "                        debug(f'New index for product_df: {new_index}')\n",
    "                        product_df.index = [new_index]\n",
    "                        e_chase_cc = pd.concat([e_chase_cc, product_df], join = 'outer')\n",
    "                        debug(f'Added product_df to e_chase_cc:\\n{e_chase_cc.tail(1)[['Date', 'Amount']]}')\n",
    "\n",
    "                    # Clear all rows from all_order_rows.\n",
    "                    debug(f'Clearing all rows from all_order_rows.')\n",
    "                    all_order_rows = all_order_rows.drop(index = all_order_rows.index)\n",
    "                    debug(f'len(all_order_rows): {len(all_order_rows)}')\n",
    "\n",
    "                    # Drop the matched Chase transaction from both Chase DataFrames.\n",
    "                    e_chase_cc = e_chase_cc.drop(index = chase_index)\n",
    "                    debug(f'Dropped matched Chase transaction (index {chase_index}) from e_chase_cc.')\n",
    "                    e_chase_cc_amazon = e_chase_cc_amazon.drop(index = chase_index)\n",
    "                    debug(f'Dropped matched Chase transaction (index {chase_index}) from e_chase_cc_amazon.')\n",
    "\n",
    "                    # Break out of the chase_index loop.\n",
    "                    debug(f'Breaking out of the chase_index loop because the whole order has been matched.')\n",
    "                    break\n",
    "\n",
    "    # If all_order_rows now contains only one row, then the prior for loop must have matched and added at \n",
    "    # least one product purchase from the order but did not find a match for the one remaining product\n",
    "    # purchase. This would be odd, and I do not expect this to happen, but in this case, there is no match \n",
    "    # for the remaining product purchase. \n",
    "    # \n",
    "    # By this point, I have tested the combined amount of all rows and the amount of each individual row. \n",
    "    # If len(all_order_rows) is now two or three, then at least one subset of rows must consist of only one \n",
    "    # row. This subset was already tested when the program attempted to match the amounts of individual rows. \n",
    "    # I assume that if at least one subset of all_order_rows must contain only one row and no single row \n",
    "    # amount matches a Chase transaction, then no other subsets of all_order_rows match a Chase transaction. \n",
    "    # This assumption could be wrong, but I do not see how Amazon would charge Emily's Chase credit card for\n",
    "    # the whole value of one part of an order but not the other(s).\n",
    "    #\n",
    "    # Based on the reasoning above, if all_order_rows now contains fewer than four rows, they have no match\n",
    "    # (unless Amazon did something odd). Therefore, add all remaining rows to unmatched_orders and proceed\n",
    "    # to the next order.\n",
    "    if len(all_order_rows) < 4:\n",
    "        debug(f'len(all_order_rows) < 4 ({len(all_order_rows)}, precisely). There is no match. Adding all remaining rows to unmatched_orders.')\n",
    "        unmatched_orders.append(all_order_rows)\n",
    "        debug(f'Appending all_order_rows to unmatched_orders.')\n",
    "        debug(f'Proceeding to the next order.')\n",
    "        continue\n",
    "\n",
    "    # Look for a match between the total Amount of all untested combinations of products and all Chase\n",
    "    # transactions with a matching date.\n",
    "    for n in range(len(all_order_rows)):\n",
    "        debug(f'Start of n loop. n = {n}')\n",
    "        \n",
    "        # Define the number of elements in the subset. The smallest subset must contain two rows.\n",
    "        r = n + 2\n",
    "        debug(f'r = {r}')\n",
    "\n",
    "        # If all_order_rows comprises multiple subsets with matching Chase transactions, at least one of those\n",
    "        # subsets must contain half or fewer of the elements of all_order_rows. Therefore, after confirming\n",
    "        # that the combined amount of all_order_rows does not match a Chase transaction, each iteration of\n",
    "        # this loop searches only for the smallest matching subset. If no subset containing half or fewer of\n",
    "        # the elements of all_order_rows matches a Chase transaction, then no larger subset will match, either\n",
    "        # (unless Amazon did something odd).\n",
    "        if r > len(all_order_rows) / 2:\n",
    "            debug(f'{r} > {len(all_order_rows) / 2}. No matching subset will be found. Breaking out of the n loop and proceeding to the next order.')\n",
    "            break\n",
    "        \n",
    "        # Define a list containing all r-length combinations of the indices of all_order_rows.\n",
    "        r_length_combinations = [combo for combo in combinations(all_order_rows.index, r)]\n",
    "        debug(f'r_length_combinations:\\n{r_length_combinations}')\n",
    "\n",
    "        # Initialize a list for storing the product purchase indices of matched transactions (to support\n",
    "        # cleaning up the r_length_combinations iterable after each pass of the chase_index loop).\n",
    "        product_purchase_indices_to_remove = []\n",
    "        \n",
    "        # Attempt to match the total Amount of each product purchase combination to the Amount of each Chase\n",
    "        # transaction in date_match_indices.\n",
    "        debug(f'Attempting to match the total Amount of each product purchase combination to the Amount of each Chase transaction in date_match_indices.')\n",
    "        for chase_index in date_match_indices:\n",
    "            debug(f'chase_index: {chase_index}')\n",
    "            chase_amount = e_chase_cc.loc[chase_index, 'Amount']\n",
    "            debug(f'chase_amount: {chase_amount}')\n",
    "            for product_combo in r_length_combinations:\n",
    "                debug(f'product_combo: {product_combo}')\n",
    "                combo_amount = all_order_rows.loc[[row for row in product_combo], 'Amount'].sum()\n",
    "                debug(f'combo_amount: {combo_amount}')\n",
    "                if chase_amount == combo_amount:\n",
    "                    debug(f'chase_amount == combo_amount')\n",
    "                \n",
    "                    # Add all product purchase rows (as DataFrames) from product_combo to e_chase_cc (with new \n",
    "                    # indices) and drop all product purchase rows from all_order_rows.\n",
    "                    for product_purchase_index in product_combo:\n",
    "                        debug(f'product_purchase_index: {product_purchase_index}')\n",
    "                        product_df = all_order_rows.loc[[product_purchase_index]].copy()\n",
    "                        debug(f'product_df:\\n{product_df[['Date', 'Amount']]}')\n",
    "                        debug(f'Final index of e_chase_cc: {e_chase_cc.index.max()}')\n",
    "                        new_index = e_chase_cc.index.max() + 1\n",
    "                        debug(f'New index for product_df: {new_index}')\n",
    "                        product_df.index = [new_index]\n",
    "                        e_chase_cc = pd.concat([e_chase_cc, product_df], join = 'outer')\n",
    "                        debug(f'Added product_df to e_chase_cc:\\n{e_chase_cc.tail(1)[['Date', 'Amount']]}')\n",
    "                        all_order_rows = all_order_rows.drop(index = product_purchase_index)\n",
    "                        debug(f'Dropped product purchase index {product_purchase_index} from all_order_rows.')\n",
    "                    debug(f'All product purchase rows in product_combo have been added to e_chase_cc and dropped from all_order_rows.')\n",
    "\n",
    "                    # Drop the matched Chase transaction from both Chase DataFrames.\n",
    "                    e_chase_cc = e_chase_cc.drop(index = chase_index)\n",
    "                    debug(f'Dropped matched Chase transaction (index {chase_index}) from e_chase_cc.')\n",
    "                    e_chase_cc_amazon = e_chase_cc_amazon.drop(index = chase_index)\n",
    "                    debug(f'Dropped matched Chase transaction (index {chase_index}) from e_chase_cc_amazon.')\n",
    "\n",
    "                    # If the program has matched all product purchases in the order, break out of the\n",
    "                    # product_combo loop. A subsequent check will break out of the chase_index loop.\n",
    "                    if len(all_order_rows) == 0:\n",
    "                        debug(f'All product purchases in order {order} have been matched to Chase transactions.')\n",
    "                        all_order_rows_matched = True\n",
    "                        debug(f'The value of all_order_rows_matched has been set to True.')\n",
    "                        debug(f'Breaking out of the product_combo loop.')\n",
    "                        break\n",
    "\n",
    "                    # Otherwise, prepare to clean up the loop iterators by appending the matched Chase index \n",
    "                    # to date_match_indices_to_remove and appending the matched product purchase indices to\n",
    "                    # product_purchase_indices_to_remove.\n",
    "                    else:\n",
    "                        debug(f'Not all product purchases in order {order} have been matched to Chase transactions. Preparing to clean up the loop iterators.')\n",
    "                        date_match_indices_to_remove.append(chase_index)                       \n",
    "                        debug(f'Chase index {chase_index} has been appended to date_match_indices_to_remove.')\n",
    "                        for i in product_combo:\n",
    "                            debug(f'Appending product purchase index {i} to product_purchase_indices_to_remove.')\n",
    "                            product_purchase_indices_to_remove.append(i)\n",
    "                    \n",
    "                    # Compare the combined Amount of all remaining rows in all_order_rows to each remaining\n",
    "                    # Chase index.\n",
    "                    debug(f'Comparing the combined Amount of all remaining rows in all_order_rows to each remaining Chase index.')\n",
    "                    total_remaining_order_amount = all_order_rows['Amount'].sum()\n",
    "                    debug(f'total_remaining_order_amount: {total_remaining_order_amount}')\n",
    "                    for chase_index_2 in date_match_indices:    # Because I currently do not use chase_index after this nested loop, using a new variable name is not strictly necessary. However, doing so is harmless and may prevent an error if I change the program in the future.\n",
    "                        debug(f'chase_index_2: {chase_index_2}')\n",
    "\n",
    "                        # Look for chase_index_2 in e_chase_cc.index. If it is there, then assign the amount\n",
    "                        # of the corresponding row to chase_amount_2. If it is not, then the corresponding row\n",
    "                        # has been removed from the DataFrame because it was matched. In this case, proceed to\n",
    "                        # the next index.\n",
    "                        if chase_index_2 in e_chase_cc.index:\n",
    "                            debug(f'chase_index_2 is in e_chase_cc.index.')\n",
    "                            chase_amount_2 = e_chase_cc.loc[chase_index_2, 'Amount']\n",
    "                            debug(f'chase_amount_2: {chase_amount_2}')\n",
    "                        else:\n",
    "                            debug(f'chase_index_2 is not in e_chase_cc.index. Proceed to the next index in date_match_indices.')\n",
    "                            continue\n",
    "                        \n",
    "                        # If there is a match, update the indicator variable and modify the data accordingly.\n",
    "                        if chase_amount_2 == total_remaining_order_amount:\n",
    "                            debug(f'chase_amount_2 == total_remaining_order_amount')\n",
    "                            all_order_rows_matched = True\n",
    "                            debug(f'The value of all_order_rows_matched has been set to True.')\n",
    "                            \n",
    "                            # Add all product purchase rows remaining in all_order_rows to e_chase_cc.\n",
    "                            debug(f'Adding all remaining product purchase rows in all_order_rows to e_chase_cc.')\n",
    "                            for product_purchase_index in all_order_rows.index:\n",
    "                                debug(f'product_purchase_index: {product_purchase_index}')\n",
    "                                product_df = all_order_rows.loc[[product_purchase_index]].copy()\n",
    "                                debug(f'product_df:\\n{product_df[['Date', 'Amount']]}')\n",
    "                                debug(f'Final index of e_chase_cc: {e_chase_cc.index.max()}')\n",
    "                                new_index = e_chase_cc.index.max() + 1\n",
    "                                debug(f'New index for product_df: {new_index}')\n",
    "                                product_df.index = [new_index]\n",
    "                                e_chase_cc = pd.concat([e_chase_cc, product_df], join = 'outer')\n",
    "                                debug(f'Added product_df to e_chase_cc:\\n{e_chase_cc.tail(1)[['Date', 'Amount']]}')\n",
    "\n",
    "                            # Drop all rows from all_order_rows.\n",
    "                            debug(f'Clearing all rows from all_order_rows.')\n",
    "                            all_order_rows = all_order_rows.drop(index = all_order_rows.index)\n",
    "                            debug(f'len(all_order_rows): {len(all_order_rows)}')\n",
    "\n",
    "                            # Drop the matched Chase transaction from both Chase DataFrames.\n",
    "                            e_chase_cc = e_chase_cc.drop(index = chase_index_2)\n",
    "                            debug(f'Dropped matched Chase transaction (index {chase_index_2}) from e_chase_cc.')\n",
    "                            e_chase_cc_amazon = e_chase_cc_amazon.drop(index = chase_index_2)\n",
    "                            debug(f'Dropped matched Chase transaction (index {chase_index_2}) from e_chase_cc_amazon.')\n",
    "                            \n",
    "                            # Since the whole order has been matched, there is no need to clean up the loop \n",
    "                            # iterators. Break out of the chase_index_2 loop.\n",
    "                            debug(f'Breaking out of the chase_index_2 loop because the whole order has been matched.')\n",
    "                            break\n",
    "\n",
    "                    # Break out of the product_combo loop.\n",
    "                    debug(f'Breaking out of the product_combo loop because the whole order has been matched.')\n",
    "                    break\n",
    "\n",
    "            # If all_order_rows_matched == True, break out of the chase_index loop.\n",
    "            if all_order_rows_matched:\n",
    "                debug(f'All product purchases in order {order} have been matched to Chase transactions. Breaking out of the chase_index loop.')\n",
    "                break\n",
    "\n",
    "            # If len(all_order_rows) < 4, then there is no match (unless len(all_order_rows) == 3 and Amazon \n",
    "            # did something odd). Break out of the chase_index loop. The final line of the order loop will\n",
    "            # add the remaining rows in all_order_rows to unmatched_orders.\n",
    "            if len(all_order_rows) < 4:\n",
    "                debug(f'len(all_order_rows) < 4 ({len(all_order_rows)}, precisely). There is no match. Breaking out of the chase_index loop.')\n",
    "                break\n",
    "\n",
    "            # Before the next pass of the chase_index loop, clean up the r_length_combinations iterator by \n",
    "            # removing all combinations containing any of the matched rows in\n",
    "            # product_purchase_indices_to_remove. Then, clear product_purchase_indices_to_remove.\n",
    "            debug(f'Cleaning up r_length_combinations by removing all combinations containing any of the matched rows in product_purchase_indices_to_remove.')\n",
    "            for i in product_purchase_indices_to_remove:\n",
    "                debug(f'Removing product purchase index {i} from r_length_combinations.')\n",
    "                for c in r_length_combinations:\n",
    "                    debug(f'Checking combination {c} for product purchase index {i}.')\n",
    "                    if i in c:\n",
    "                        debug(f'Product purchase index {i} is in combination {c}. Removing combination {c} from r_length_combinations.')\n",
    "                        r_length_combinations.remove(c)\n",
    "            debug(f'Updated r_length_combinations:\\n{r_length_combinations}')\n",
    "            product_purchase_indices_to_remove.clear()\n",
    "            debug(f'Cleared product_purchase_indices_to_remove.')\n",
    "\n",
    "        # If all_order_rows_matched == True, break out of the n loop.\n",
    "        if all_order_rows_matched:\n",
    "            debug(f'All product purchases in order {order} have been matched to Chase transactions. Breaking out of the n loop.')\n",
    "            break\n",
    "\n",
    "        # If len(all_order_rows) < 4, break out of the n loop.\n",
    "        if len(all_order_rows) < 4:\n",
    "            debug(f'len(all_order_rows) < 4 ({len(all_order_rows)}, precisely). There is no match. Breaking out of the n loop.')\n",
    "            break\n",
    "\n",
    "        # Clean up the date_match_indices iterator before the next pass of the n loop by removing any elements\n",
    "        # in date_match_indices_to_remove. Then, clear date_match_indices_to_remove.\n",
    "        debug(f'Cleaning up date_match_indices by removing all matched indices in date_match_indices_to_remove.')\n",
    "        for i in date_match_indices_to_remove:\n",
    "            debug(f'Removing matched index {i} from date_match_indices.')\n",
    "            date_match_indices.remove(i)\n",
    "\n",
    "    # If, at this point, len(all_order_rows) > 0, then there is no match for the remaining rows. Therefore, \n",
    "    # append all remaining rows to unmatched_orders.\n",
    "    if len(all_order_rows) > 0:\n",
    "        debug(f'len(all_order_rows) > 0 ({len(all_order_rows)}, precisely). There is no match for the remaining rows. Appending all remaining rows to unmatched_orders.')\n",
    "        unmatched_orders.append(all_order_rows)\n",
    "        debug(f'Appended all_order_rows to unmatched_orders.')\n",
    "\n",
    "# Compile the elements of unmatched_orders into a DataFrame and export it as a CSV for review.\n",
    "debug(f'Compiling unmatched_orders into a DataFrame.')\n",
    "unmatched_orders = pd.concat(unmatched_orders, ignore_index = True)\n",
    "debug(f'Exporting unmatched_orders to CSV.')\n",
    "unmatched_orders.to_csv(f'Data/3. Final/{TODAY} Unmatched Amazon Orders.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many product purchase rows did the program add to e_chase_cc?\n",
    "len(e_chase_cc[e_chase_cc['Account'] == 'Amazon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many product purchase rows were in amazon_data_chase?\n",
    "len(amazon_data_chase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the difference?\n",
    "len(amazon_data_chase) - len(e_chase_cc[e_chase_cc['Account'] == 'Amazon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does this match the number of rows in unmatched_orders?\n",
    "len(unmatched_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Combining, Exporting, and Importing Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data into a single DataFrame.\n",
    "\n",
    "all_transactions = pd.concat([z_wf_ch, e_ba_ch, e_ba_cc, e_chase_cc], ignore_index = True)\n",
    "\n",
    "\n",
    "# Export the data for manual analysis.\n",
    "\n",
    "all_transactions.to_csv(f'Data/2. Intermediate/{TODAY} All Transactions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the reviewed data back into the program.\n",
    "\n",
    "all_transactions = pd.read_csv(f'Data/3. Final/{TODAY} All Transactions.csv')    # Replace the date with {TODAY}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new data to the historical data file.\n",
    "\n",
    "historical_data = pd.read_csv('Data/0. Historical/Historical Data.csv')\n",
    "historical_data = pd.concat([historical_data, all_transactions], ignore_index = True)\n",
    "historical_data.to_csv(f'Data/0. Historical/{TODAY} Historical Data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize total net spending by spender as a pie chart.\n",
    "\n",
    "display_pie_chart(all_transactions, 'Spender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize net spending by category as a pie chart.\n",
    "\n",
    "display_bar_chart(all_transactions, 'Category')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
